{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "408118a4",
   "metadata": {},
   "source": [
    "# Glove - 126161\n",
    "\n",
    "his notebook implements the GloVe model from scratch using a weighted least-squares objective over global word co-occurrence statistics, trained on a subset of the Reuters-21578 corpus with a dynamic context window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "968b29b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e296e4f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     /Users/michaellacar/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/michaellacar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/michaellacar/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "for res in [\"reuters\", \"punkt\", \"punkt_tab\"]:\n",
    "    try:\n",
    "        nltk.data.find(res)\n",
    "    except LookupError:\n",
    "        nltk.download(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b96ac59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_reuters():\n",
    "    corpus = []\n",
    "    for fid in reuters.fileids():\n",
    "        text = reuters.raw(fid).lower()\n",
    "        text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
    "        tokens = word_tokenize(text)\n",
    "        if len(tokens) > 2:\n",
    "            corpus.append(tokens)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "400b6239",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = load_reuters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "295dbdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_DOCS = 300   # recommended: 200â€“300\n",
    "corpus = corpus[:MAX_DOCS]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb32d097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 1190\n"
     ]
    }
   ],
   "source": [
    "MIN_COUNT = 5\n",
    "\n",
    "all_tokens = [word for doc in corpus for word in doc]\n",
    "word_counts = Counter(all_tokens)\n",
    "\n",
    "vocabs = [word for word, count in word_counts.items() if count >= MIN_COUNT]\n",
    "\n",
    "word2index = {word: idx for idx, word in enumerate(vocabs)}\n",
    "index2word = {idx: word for word, idx in word2index.items()}\n",
    "\n",
    "vocab_size = len(vocabs)\n",
    "print(\"Vocabulary size:\", vocab_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e3d45e",
   "metadata": {},
   "source": [
    "# Build Co-occurence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "653aa785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of co-occurrence pairs: 46351\n"
     ]
    }
   ],
   "source": [
    "def build_cooccurrence(corpus, window_size=2):\n",
    "    cooc = defaultdict(float)\n",
    "\n",
    "    for doc in corpus:\n",
    "        for i, word in enumerate(doc):\n",
    "            if word not in word2index:\n",
    "                continue\n",
    "\n",
    "            center_id = word2index[word]\n",
    "\n",
    "            start = max(0, i - window_size)\n",
    "            end = min(len(doc), i + window_size + 1)\n",
    "\n",
    "            for j in range(start, end):\n",
    "                if i != j and doc[j] in word2index:\n",
    "                    context_id = word2index[doc[j]]\n",
    "                    distance = abs(i - j)\n",
    "                    cooc[(center_id, context_id)] += 1.0 / distance\n",
    "\n",
    "    return cooc\n",
    "\n",
    "window_size = 2\n",
    "cooc_matrix = build_cooccurrence(corpus, window_size)\n",
    "print(\"Number of co-occurrence pairs:\", len(cooc_matrix))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531a530c",
   "metadata": {},
   "source": [
    "# Glove Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b771d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloVe(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, emb_size):\n",
    "        super().__init__()\n",
    "        self.wi = nn.Embedding(vocab_size, emb_size)\n",
    "        self.wj = nn.Embedding(vocab_size, emb_size)\n",
    "        self.bi = nn.Embedding(vocab_size, 1)\n",
    "        self.bj = nn.Embedding(vocab_size, 1)\n",
    "\n",
    "    def forward(self, i_idx, j_idx, x_ij):\n",
    "        wi = self.wi(i_idx)\n",
    "        wj = self.wj(j_idx)\n",
    "        bi = self.bi(i_idx).squeeze()\n",
    "        bj = self.bj(j_idx).squeeze()\n",
    "\n",
    "        dot = torch.sum(wi * wj, dim=1)\n",
    "        loss = (dot + bi + bj - torch.log(x_ij)) ** 2\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3011cbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighting_function(x, x_max=100, alpha=0.75):\n",
    "    return torch.where(\n",
    "        x < x_max,\n",
    "        (x / x_max) ** alpha,\n",
    "        torch.ones_like(x)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea5808eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = list(cooc_matrix.keys())\n",
    "values = list(cooc_matrix.values())\n",
    "\n",
    "pairs = torch.LongTensor(pairs)\n",
    "values = torch.FloatTensor(values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e917ecb3",
   "metadata": {},
   "source": [
    "# Training configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8098528",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 2\n",
    "learning_rate = 0.05\n",
    "num_epochs = 30\n",
    "batch_size = 512\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bff3dc4",
   "metadata": {},
   "source": [
    "# Train Glove Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18352aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30] Loss: 10.4356\n",
      "Epoch [2/30] Loss: 3.8168\n",
      "Epoch [3/30] Loss: 3.1627\n",
      "Epoch [4/30] Loss: 2.9171\n",
      "Epoch [5/30] Loss: 2.7187\n",
      "Epoch [6/30] Loss: 2.5554\n",
      "Epoch [7/30] Loss: 2.4565\n",
      "Epoch [8/30] Loss: 2.4008\n",
      "Epoch [9/30] Loss: 2.3724\n",
      "Epoch [10/30] Loss: 2.3503\n",
      "Epoch [11/30] Loss: 2.3428\n",
      "Epoch [12/30] Loss: 2.3254\n",
      "Epoch [13/30] Loss: 2.3229\n",
      "Epoch [14/30] Loss: 2.3522\n",
      "Epoch [15/30] Loss: 2.3466\n",
      "Epoch [16/30] Loss: 2.3359\n",
      "Epoch [17/30] Loss: 2.3550\n",
      "Epoch [18/30] Loss: 2.3463\n",
      "Epoch [19/30] Loss: 2.3211\n",
      "Epoch [20/30] Loss: 2.3397\n",
      "Epoch [21/30] Loss: 2.3418\n",
      "Epoch [22/30] Loss: 2.3564\n",
      "Epoch [23/30] Loss: 2.3263\n",
      "Epoch [24/30] Loss: 2.3479\n",
      "Epoch [25/30] Loss: 2.3559\n",
      "Epoch [26/30] Loss: 2.3414\n",
      "Epoch [27/30] Loss: 2.3591\n",
      "Epoch [28/30] Loss: 2.3317\n",
      "Epoch [29/30] Loss: 2.3829\n",
      "Epoch [30/30] Loss: 2.3523\n",
      "GloVe Loss: 0.0261\n",
      "GloVe Training Time: 0.77 seconds\n"
     ]
    }
   ],
   "source": [
    "model = GloVe(vocab_size, embedding_dim)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "num_batches = len(values) // batch_size\n",
    "\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "\n",
    "    perm = torch.randperm(len(values))\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        idx = perm[i * batch_size:(i + 1) * batch_size]\n",
    "\n",
    "        i_idx = pairs[idx][:, 0]\n",
    "        j_idx = pairs[idx][:, 1]\n",
    "        x_ij = values[idx]\n",
    "\n",
    "        weights = weighting_function(x_ij)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(i_idx, j_idx, x_ij)\n",
    "        loss = torch.mean(weights * loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {total_loss:.4f}\")\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "glove_loss = total_loss / num_batches\n",
    "glove_time = end_time - start_time\n",
    "\n",
    "print(f\"GloVe Loss: {glove_loss:.4f}\")\n",
    "print(f\"GloVe Training Time: {glove_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ad2c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = (\n",
    "    model.wi.weight.detach().cpu().numpy() +\n",
    "    model.wj.weight.detach().cpu().numpy()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff1f6e6",
   "metadata": {},
   "source": [
    "# Plot Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c741229b",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_K = 50\n",
    "top_words = [w for w, _ in word_counts.most_common(TOP_K) if w in word2index]\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "for word in top_words:\n",
    "    idx = word2index[word]\n",
    "    x, y = embeddings[idx]\n",
    "    plt.scatter(x, y)\n",
    "    plt.text(x, y, word, fontsize=9)\n",
    "\n",
    "plt.title(\"GloVe Word Embeddings (Reuters Corpus)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
