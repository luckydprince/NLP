{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b9c79df",
   "metadata": {},
   "source": [
    "# st126161\n",
    "# A2: Language Model using LSTM\n",
    "## Harry Potter (7-Book Dataset)\n",
    "\n",
    "This notebook implements a character-level LSTM language model trained on\n",
    "the complete Harry Potter book series (7 books) obtained from Kaggle.\n",
    "\n",
    "The model is trained using GPU (CUDA) acceleration for improved performance\n",
    "and is capable of generating coherent text based on a user-provided prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "152bc889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "print(torch.__version__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca8de810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Device name: Apple Silicon GPU (MPS)\n"
     ]
    }
   ],
   "source": [
    "''' # Check CUDA availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Optional: show GPU name\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0)) '''\n",
    "\n",
    "# I'm using the M4 chip on a MacBook Air\n",
    "\n",
    "# Device selection: CUDA (NVIDIA), MPS (Apple Silicon), or CPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    device_name = torch.cuda.get_device_name(0)\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    device_name = \"Apple Silicon GPU (MPS)\"\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    device_name = \"CPU\"\n",
    "\n",
    "print(\"Using device:\", device)\n",
    "print(\"Device name:\", device_name)\n",
    "\n",
    "torch.mps.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eef62b8",
   "metadata": {},
   "source": [
    "# Load data\n",
    "\n",
    "Data was downloaded from Kaggle (https://www.kaggle.com/datasets/shubhammaindola/harry-potter-books?resource=download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c21316b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters in dataset: 6285445\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"harry_potter_books\"\n",
    "\n",
    "all_text = \"\"\n",
    "for filename in sorted(os.listdir(data_dir)):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        with open(os.path.join(data_dir, filename), \"r\", encoding=\"utf-8\") as f:\n",
    "            all_text += f.read() + \"\\n\"\n",
    "\n",
    "print(\"Total characters in dataset:\", len(all_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d1ee1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = all_text.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97a68552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Dataset Sample (First 1,000 characters) ===\n",
      "\n",
      "m r. and mrs. dursley, of number four, privet drive, were proud to say that they were perfectly normal, thank you very much. they were the last people you’d expect to be involved in anything strange or mysterious, because they just didn’t hold with such nonsense.\n",
      "\n",
      "mr. dursley was the director of a firm called grunnings, which made drills. he was a big, beefy man with hardly any neck, although he did have a very large mustache. mrs. dursley was thin and blonde and had nearly twice the usual amount of neck, which came in very useful as she spent so much of her time craning over garden fences, spying on the neighbors. the dursleys had a small son called dudley and in their opinion there was no finer boy anywhere.\n",
      "\n",
      "the dursleys had everything they wanted, but they also had a secret, and their greatest fear was that somebody would discover it. they didn’t think they could bear it if anyone found out about the potters. mrs. potter was mrs. dursley’s sister, but they hadn’t met for several ye\n"
     ]
    }
   ],
   "source": [
    "# Display a short glimpse of the dataset\n",
    "print(\"=== Dataset Sample (First 1,000 characters) ===\\n\")\n",
    "print(text[:1000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7584b28e",
   "metadata": {},
   "source": [
    "A brief inspection of the dataset was conducted by printing sample paragraphs from the combined corpus to verify textual integrity and narrative continuity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a80e9b1",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37813935",
   "metadata": {},
   "source": [
    "## Character level tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a29f029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 80\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "char2idx = {c: i for i, c in enumerate(chars)}\n",
    "idx2char = {i: c for c, i in char2idx.items()}\n",
    "\n",
    "print(\"Vocabulary size:\", vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbe8b99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary saved successfully\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"vocab.pkl\", \"wb\") as f:\n",
    "    pickle.dump((char2idx, idx2char), f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "print(\"Vocabulary saved successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c787536",
   "metadata": {},
   "source": [
    "## Encode text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "17094d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text = np.array([char2idx[c] for c in text])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78018eb",
   "metadata": {},
   "source": [
    "## Create input target sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "931364f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([6285405, 40])\n",
      "Target shape: torch.Size([6285405])\n"
     ]
    }
   ],
   "source": [
    "seq_length = 40\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in range(len(encoded_text) - seq_length):\n",
    "    X.append(encoded_text[i:i + seq_length])\n",
    "    y.append(encoded_text[i + seq_length])\n",
    "\n",
    "X = torch.tensor(X, dtype=torch.long)\n",
    "y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "print(\"Input shape:\", X.shape)\n",
    "print(\"Target shape:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7af5c7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.to(device)\n",
    "y = y.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15100b0f",
   "metadata": {},
   "source": [
    "# Define LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1cde432f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5a63f347",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 128\n",
    "hidden_size = 256\n",
    "\n",
    "model = LSTMLanguageModel(vocab_size, embed_size, hidden_size).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.003)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7f5c3a",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaadf38a",
   "metadata": {},
   "source": [
    "Due to the large size of the combined 7-book corpus, i utilized mini-batch training using PyTorch’s DataLoader to avoid memory overflow and to enable efficient GPU acceleration on Apple Silicon via MPS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d0c93988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.4028\n",
      "Epoch [2/10], Loss: 1.3477\n",
      "Epoch [3/10], Loss: 1.3890\n",
      "Epoch [4/10], Loss: 1.4660\n",
      "Epoch [5/10], Loss: 1.5186\n",
      "Epoch [6/10], Loss: 1.5714\n",
      "Epoch [7/10], Loss: 1.5304\n",
      "Epoch [8/10], Loss: 1.5491\n",
      "Epoch [9/10], Loss: 1.5397\n",
      "Epoch [10/10], Loss: 1.5287\n",
      "Training time (seconds): 13460.781935930252\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "batch_size = 128  # Safe for Apple Silicon\n",
    "\n",
    "dataset = TensorDataset(X, y)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "epochs = 10\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for batch_X, batch_y in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    avg_loss = epoch_loss / len(dataloader)\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Training time (seconds):\", end_time - start_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620606ea",
   "metadata": {},
   "source": [
    "# Text Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4f608711",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def generate_text(model, start_text, gen_length=200, temperature=0.8):\n",
    "    model.eval()\n",
    "\n",
    "    input_seq = torch.tensor(\n",
    "        [char2idx[c] for c in start_text.lower()],\n",
    "        dtype=torch.long\n",
    "    ).unsqueeze(0).to(device)\n",
    "\n",
    "    generated_text = start_text\n",
    "\n",
    "    for _ in range(gen_length):\n",
    "        output = model(input_seq)\n",
    "\n",
    "        # Apply temperature\n",
    "        logits = output / temperature\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "\n",
    "        # Sample instead of argmax\n",
    "        next_char_idx = torch.multinomial(probs, 1).item()\n",
    "        generated_text += idx2char[next_char_idx]\n",
    "\n",
    "        input_seq = torch.cat(\n",
    "            [input_seq[:, 1:], torch.tensor([[next_char_idx]]).to(device)],\n",
    "            dim=1\n",
    "        )\n",
    "\n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdb05d8",
   "metadata": {},
   "source": [
    "# Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fd3d6628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "harry potter is always a probling the said on the peciar, where she to be as they seen the sead him.\n",
      "\n",
      "“all you closed looking them, complacing with her people. puns minders. hermione what she town. he know.”\n",
      "\n",
      "harry had behnied or ell, his has better. that had see angling harry was lused to the dox crabbe chick to \n"
     ]
    }
   ],
   "source": [
    "prompt = \"harry potter is\"\n",
    "print(generate_text(model, prompt, gen_length=300, temperature=0.8))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a9c63a",
   "metadata": {},
   "source": [
    "Initial text generation using greedy decoding resulted in repetitive outputs due to the model favoring high-probability character sequences. To address this, temperature-based probabilistic sampling was implemented, which significantly improved output diversity and reduced repetition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b1bf3b",
   "metadata": {},
   "source": [
    "The text data preprocessing began by loading and merging the seven Harry Potter book files into a single corpus. The text was normalized by converting all characters to lowercase to reduce vocabulary size and improve training stability. A brief dataset inspection was performed by printing sample paragraphs to verify textual integrity. Character-level tokenization was then applied by identifying all unique characters and mapping them to integer indices. The entire corpus was encoded numerically, after which fixed-length input sequences were generated, where each sequence was paired with the subsequent character as the prediction target. Finally, the data was converted into PyTorch tensors and transferred to the appropriate computation device."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f1f816",
   "metadata": {},
   "source": [
    "A character-level LSTM language model was implemented using PyTorch. The model consists of an embedding layer that transforms character indices into dense vectors, followed by an LSTM layer that captures sequential dependencies within the text. The final fully connected layer projects the LSTM output to the vocabulary space to predict the next character. The model was trained using a cross-entropy loss function and optimized with the Adam optimizer. Due to the large dataset size, mini-batch training was employed using a DataLoader to ensure efficient memory usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae9a4c5",
   "metadata": {},
   "source": [
    "# Exporting the model for web application use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5dd2e407",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"harry_potter_lstm.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
